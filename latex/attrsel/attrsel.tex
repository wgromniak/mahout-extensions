\documentclass[12pt]{pracamgr}
\linespread{1.2}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}

\newtheorem{definition}{Definition}[chapter]
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{remark}{Remark}[chapter]
\newtheorem{corollary}{Corollary}[chapter]
\newtheorem{lemma}{Lemma}[chapter]

% \theoremstyle{definition}
\newtheorem{statement}{Statement}[chapter]
\newtheorem{example}{Example}[chapter]

\title{Scalability of attribute selection methods: Application of rough sets and MapReduce}
\author{Wiktor Gromniak}
\nralbumu{311643}
\date{May 2015}

\opiekun{prof. Andrzej Skowron\\ Section of Mathematical Logic\\}

\dziedzina{11.1 Mathematics\\}

\klasyfikacja{TODO}

\keywords{TODO}

\tytulang{Skalowalność metod selekcji atrybutów: Zastosowanie  zbiorów przybliżonych i \\
MapReduce}

\begin{document}
\maketitle
\tableofcontents

\begin{abstract}
TODO
\end{abstract}

\chapter{Introduction}

[TODO: in the main thesis Introduction write about DRACUS, UCL, etc.]

Attribute selection is a machine learning task, whose goal is to select relevant attributes from a given (usually large) set, which usually precedes construction of a classifier or other prediction model from the selected subset of attributes. It is most useful in the domains where tens of thousands of attributes occur, such as processing of text documents, analysis of gene expression microarrays or combinatorial chemistry \cite{guyon} [TODO: more refs available]. A model with lower number of attributes has better performance, is faster and more memory-efficient and it is easier to find it's explanation in the domain language \cite{guyon}. The task of attribute selection is described in detail in chapter \ref{chap:attrsel}.

To accomplish the attribute selection task we use the notions of reducts and more specifically dynamic and random reducts [TODO: cite]. They are based on the rough set theory and boolean reasoning [TODO: cite]. We will introduce the basic concepts of rough sets and reducts in chapter \ref{chap:roughsets} and provide a unified approach towards dynamic and random reducts in chapter \ref{chap:dynamicrandomreducts}. We will further show how to use these notions in attribute selection in chapter \ref{chap:reductsattrsel}.

As the datasets explored in attribute selection are usually very large and algorithms used to process them computationally expensive, we seek to make them more scalable using the recently developed paradigms capable of processing large amounts of data in distributed computing environments, namely MapReduce and Apache Spark\footnote{[TODO: find out how to name Spark's computing paradigm.]}. These notions, their comparison are introduced in detail in chapter [TODO: cite Krzysztof's BigData section]. The distributed attribute selection algorithms, their characteristics and computational complexity are introduced and analysed in chapter \ref{chap:implattlsel}. We seek to provide our algorithms to the broader audience by providing their implementations open source as extensions to existing machine learning libraries. [TODO: document this]

We further perform a series of experiments on artificial and freely available, real world data to asses the scalability and performance of our algorithms. The results are discussed in chapter \ref{chap:experiments}. We also compare the predictive power of the models obtained using our algorithms with other state of the art methodologies.

[TODO: DRACUS if we manage]

\chapter{Attribute selection task}
\label{chap:attrsel}

\begin{itemize}
  \item information systems, attributes, etc.
  \item what attribute selection is
  \item the assumption of redundant and irrelevant features
  \item benefits and areas of utilisation
\end{itemize}

\chapter{Rough sets preliminaries}
\label{chap:roughsets}

The theoretical, machine learning concept we are going to use are rough sets - a concept introduced by Pawlak in \cite{pawlak}. We will first define the basic concepts underlying
this theory\footnote{In the definitions we will most closely follow [TODO cite Bazan].}.

Let us introduce the basic notion, which we have already used informally in the previous chapter - \emph{the information system } [TODO: cite 111 from Son hab].

\begin{definition}[information system]

\emph{Information system}\footnote{Also referred to simply as \emph{decision table}.}, is a 
pair $\mathcal{A} = (U, A \cup \{d\})$, where
  \begin{itemize}
    \item $\textrm{U}$ is a finite set, whose elements will be referred to as \emph{objects}
    \item $A$ is a set of \emph{attributes}, such that $a \in A$ is a function
    $a : U \to V^a$, ($V^a$ - set of the attribute's values),
    \item $d$ is a distinguished \emph{decision attribute}.
  \end{itemize}

\end{definition}

Information system may be graphically represented as a table, an example of which we have 
already seen in Table [TODO], where objects are placed in rows of the table and 
attributes are put into columns, i.e. entry in $i$-th row and $j$-th column corresponds
to $j$-th attribute's value on $i$-th object.

\begin{definition}[indiscernibility relation]

  Let $\mathcal{A} = (U, A \cup \{d\})$  and $P \subseteq A$. Then we will call
  $$
    IND_\mathcal{A}(P) = \{(x, y) \in U \times U : \forall_{a \in P} \, a(x) = a(y) \}
  $$
  \emph{indiscernibility relation, generated by the set $P$}.

\end{definition}

It's easy to check that this relation is an equivalence relation. The definition says that
two objects $a_1, a_2 \in A$ are in the relation $IND_\mathcal{A}(P)$ when they have the
same values of attributes, for attributes from the set $P$. This relation contains
information about 

As the decision attribute is usually of the most interest for us, we would like to 

\begin{definition}[decision indiscernibility relation]

  Let $\mathcal{A} = (U, A \cup \{d\})$  and $P \subseteq A$. Then we will call
  $$
    IND_\mathcal{A}(P, d) = \{(x, y) \in U \times U : (x,y) \in IND_\mathcal{A}(P)\\
    \textsf{ lub } d(x) = d(y)\}
  $$
  \emph{decision} indiscernibility relation, generated by the set $P$.

\end{definition}

\begin{definition}[(non)redundant attributes, independent subsets of attributes]

  Let $\mathcal{A} = (U, A \cup \{d\})$  and $P \subseteq A$.
  \begin{itemize}
    \item We will call $a \in A$ \emph{redundant}, if 
    $IND_\mathcal{A}(P) = IND_\mathcal{A}(P \setminus\{a\})$, otherwise \emph{nonredundant}.
    \item $P$ is \emph{independent} in $\mathcal{A}$, if every attribute in $P$ is non-
    redundant in $P$.
  \end{itemize}

\end{definition}

"Huh, we were already talking about redundancy!" you say. Yes, that's the clue mate!

\begin{definition}[reducts]

  Let $\mathcal{A} = (U, A \cup \{d\})$  and $P \subseteq A$. Then, a \emph{decision reduct
  of the information system $\mathcal{A}$} (denoted. $RED(\mathcal{A}, d)$) is such a
  subset $Q \subseteq A$, that
  \begin{itemize}
    \item the set $Q$ is independent in $\mathcal{A}$,
    \item $IND_\mathcal{A}(A, d) = IND_\mathcal{A}(Q, d)$
  \end{itemize}

\end{definition}




\chapter{Unified approach towards dynamic and random reducts}
\label{chap:dynamicrandomreducts}

\begin{definition}[object subtable]

  Let $\mathcal{A} = (U, A \cup \{d\})$ - decision table, where $A = \{a_1, \dots, a_m\}$. 
  We will call a pair $\mathcal{A'}  = (U', A' \cup \{d\})$, where 
  \begin{itemize}
    \item $U' \subseteq U$,
    \item $d' = d |_{U'}$,
    \item $A' = \{a_1', \dots,  a_m'\}$ and $a_i' = a_i |_{U'}$, $i = 1, 2, \dots, m$
  \end{itemize}
  an \emph{object subtable} of table $\mathcal{A}$.
  
\end{definition}

Notice that the number off attributes is exactly the same in both: the original table $\mathcal{A}$ and the object subtable $\mathcal{A'}$. They were just given different names, 
for formally they are different functions (recall how we've defined attributes as functions
in [def].

\begin{definition}[attribute subtable]

  Let $\mathcal{A} = (U, A \cup \{d\})$ - decision table, where $A = \{a_1, \dots, a_m\}$. 
  We will call a pair $\mathcal{A'}  = (U, A' \cup \{d\})$, where 
  \begin{itemize}
    \item $d' = d |_{U'}$,
    \item $A' \subseteq A$
  \end{itemize}
  an \emph{attribute subtable} of table $\mathcal{A}$.
  
\end{definition}

\begin{remark}

  $P_O(\mathcal{A})$ and $P_A(\mathcal{A})$ will denote the families of all object and
  attribute, respectively, subtables of the table $\mathcal{A}$.

\end{remark}

In the following definitions we won't distinguish between object and attribute subtables,
since they work for both. Thus, we will simply write \emph{subtable} in each case.

\begin{definition}[$F$-dynamic reducts]

  Let $\mathcal{A} = (U, A \cup \{d\})$ and $\mathcal{F} \subseteq P(\mathcal{A})$. Then
  $$
  GDR(\mathcal{A},\mathcal{F}) = \bigcap_{\mathcal{B} \in \mathcal{F}} RED(\mathcal{B}, D)
  $$
  is a set of \emph{generalised $F$-dynamic reducts}.
  
\end{definition}

\begin{definition}[$(F,\varepsilon)$-dynamic reducts]

  Let $\mathcal{A} = (U, A \cup \{d\})$, $\mathcal{F} \subseteq P(\mathcal{A})$ and
  $\varepsilon \in [0, 1]$. Then
  \begin{align*}
  GDR_\varepsilon(\mathcal{A},\mathcal{F}) = \{C \subseteq A :
  \frac{|\{\mathcal{B} \in \mathcal{F} : C \in RED(\mathcal{B}, d)\}|}{|\mathcal{F}|} \geq 
  1 - \varepsilon \}
  \end{align*}
  is a set of \emph{generalised $(F,\varepsilon)$-dynamic reducts}.
  
\end{definition}

\begin{definition}[stability factor]

  For $R \in RED(\mathcal{B},d)$, $\mathcal{B} \in \mathcal{F}$, we will call the number
  $$ \frac{|\{\mathcal{B} \in \mathcal{F} : R \in RED(\mathcal{B}, d)\}|}{|\mathcal{F}|} $$
  a \emph{stability factor} of the reduct $R$.
  
\end{definition}


\chapter{Reducts-based attribute selection}
\label{chap:reductsattrsel}

\chapter{MapReduce and Apache Spark}
\label{chap:mapredspark}
  
  
\chapter{Implementation in big data frameworks}
\label{chap:implattlsel}

\section{MapRecude (Hadoop)}

\section{Apache Spark}

Spark is capable of loading all the data into the cluster's memory, thus it is well suited for machine learning algorithms \cite{zaharia}.


\chapter{Experiments}
\label{chap:experiments}

\section{Scalability}
\subsection{Comparison of various implementations}

Spark $\gg$ MapReduce.

\section{Predictive performance and comparison to other methods}

\chapter{Conclusions}

We've worked hard and achieved very little.

\begin{thebibliography}{9}
\addcontentsline{toc}{chapter}{Bibliography}

% this guy is the author of spark
% from wikipedia "Apache Spark"
\bibitem{zaharia} Matei Zaharia, \emph{Spark: In-Memory Cluster Computing for Iterative and
                  Interactive Applications}, Invited Talk at NIPS 2011 Big Learning
                  Workshop: Algorithms, Systems, and Tools for Learning at Scale.
                  \footnote{[TODO: find a paper]}
                  
% from our Dropbox
\bibitem{guyon} Isabelle Guyon, Andre Elisseef, \emph{An Introduction to Variable and
                Feature Selection}
                
\bibitem{pawlak} Zdzis\l{}aw Pawlak, Rough sets

\end{thebibliography}
\end{document}
